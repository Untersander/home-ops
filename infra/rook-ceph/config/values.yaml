# Installs a debugging toolbox deployment
toolbox:
  # -- Enable Ceph debugging pod deployment. See [toolbox](../Troubleshooting/ceph-toolbox.md)
  enabled: true

monitoring:
  # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
  # Monitoring requires Prometheus to be pre-installed
  enabled: true
  # -- Whether to create the Prometheus rules for Ceph alerts
  createPrometheusRules: true
  # -- Edit Prometheus rules for Ceph alerts

# All values below are taken from the CephCluster CRD
# -- Cluster configuration.
# @default -- See [below](#ceph-cluster-spec)
cephClusterSpec:
  # For more details, check https://rook.io/docs/rook/v1.18/CRDs/Cluster/ceph-cluster-crd/

  # The path on the host where configuration files will be persisted. Must be specified. If there are multiple clusters, the directory must be unique for each cluster.
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
  dataDirHostPath: /var/mnt/local-system-sdd/lib/rook

  mon:
    # Set the number of mons to be started. Generally recommended to be 3.
    # For highest availability, an odd number of mons should be specified.
    count: 1
    # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
    # Mons should only be allowed on the same node for test environments where data loss is acceptable.
    allowMultiplePerNode: false

  mgr:
    # When higher availability of the mgr is needed, increase the count to 2.
    # In that case, one mgr will be active and one in standby. When Ceph updates which
    # mgr is active, Rook will update the mgr services to match the active mgr.
    count: 1
    allowMultiplePerNode: false

  # enable the ceph dashboard for viewing cluster status
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
    # serve the dashboard at the given port.
    # port: 8443
    # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
    # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
    ssl: false

  # Network configuration, see: https://github.com/rook/rook/blob/master/Documentation/CRDs/Cluster/ceph-cluster-crd.md#network-configuration-settings
  network:
    # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
    ipFamily: "IPv6"
    # Ceph daemons to listen on both IPv4 and Ipv6 networks
    dualStack: true

  # enable the crash collector for ceph daemon crash collection
  crashCollector:
    disable: false
    # Uncomment daysToRetain to prune ceph crash entries older than the
    # specified number of days.
    daysToRetain: 30

  # enable log collector, daemons will log on files and rotate
  logCollector:
    enabled: true
    periodicity: daily # one of: hourly, daily, weekly, monthly
    maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1M.

  # The option to automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: false

  # priority classes to apply to ceph resources
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical

  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    # deviceFilter:
    # config:
    #   crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
    #   metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
    #   databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
    #   osdsPerDevice: "1" # this value can be overridden at the node or device level
    #   encryptedDevice: "true" # the default value for this option is "false"
    # # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
    # # nodes below will be used as storage resources. Each node's 'name' field should match their 'kubernetes.io/hostname' label.
    # nodes:
    #   - name: "172.17.4.201"
    #     devices: # specific devices to use for storage can be specified for each node
    #       - name: "sdb"
    #       - name: "nvme01" # multiple osds can be created on high performance devices
    #         config:
    #           osdsPerDevice: "5"
    #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
    #     config: # configuration can be specified at the node level which overrides the cluster level config
    #   - name: "172.17.4.301"
    #     deviceFilter: "^sd."
    nodes:
      - name: "cp-0"
        devices: # specific devices to use for storage can be specified for each node
          - name: "sda"
          - name: "sdb"
          - name: "sdc"

# ingress:
#   # -- Enable an ingress for the ceph-dashboard
#   dashboard:
    # {}
    # labels:
    #   external-dns/private: "true"
    # annotations:
    #   external-dns.alpha.kubernetes.io/hostname: dashboard.example.com
    #   nginx.ingress.kubernetes.io/rewrite-target: /ceph-dashboard/$2
    # If the dashboard has ssl: true the following will make sure the NGINX Ingress controller can expose the dashboard correctly
    #   nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
    #   nginx.ingress.kubernetes.io/server-snippet: |
    #     proxy_ssl_verify off;
    # host:
    #   name: dashboard.example.com
    #   path: "/ceph-dashboard(/|$)(.*)"
    #   pathType: Prefix
    # tls:
    # - hosts:
    #     - dashboard.example.com
    #   secretName: testsecret-tls
    ## Note: Only one of ingress class annotation or the `ingressClassName:` can be used at a time
    ## to set the ingress class
    # ingressClassName: nginx

route:
  # -- Enable an HTTPRoute for the ceph-dashboard
  dashboard:
    # labels:
    #   external-dns/private: "true"
    # annotations:
    #   external-dns.alpha.kubernetes.io/hostname: dashboard.example.com
    #   nginx.ingress.kubernetes.io/rewrite-target: /ceph-dashboard/$2
    host:
      name: ceph-dashboard.k8s.garden
      path: "/"
      pathType: PathPrefix
    parentRefs:
      - name: gateway-shared
        namespace: kube-system
        sectionName: https

# -- A list of CephBlockPool configurations to deploy
# @default -- See [below](#ceph-block-pools)
cephBlockPools:
  - name: ceph-blockpool-single
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
    spec:
      failureDomain: osd
      replicated:
        size: 1
      # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
      # For reference: https://docs.ceph.com/docs/latest/mgr/prometheus/#rbd-io-statistics
      # enableRBDStats: true
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "WaitForFirstConsumer"

# -- A list of CephFileSystem configurations to deploy
# @default -- See [below](#ceph-file-systems)
cephFileSystems:
  - name: ceph-filesystem
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - failureDomain: osd
          replicated:
            size: 2
          # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
          name: data0
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            memory: "4Gi"
          requests:
            cpu: "1000m"
            memory: "4Gi"
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: true
      name: ceph-filesystem
      # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "WaitForFirstConsumer"

# -- Settings for the filesystem snapshot class
# @default -- See [CephFS Snapshots](../Storage-Configuration/Ceph-CSI/ceph-csi-snapshot.md#cephfs-snapshots)
cephFileSystemVolumeSnapshotClass:
  enabled: false
  name: ceph-filesystem
  isDefault: true
  deletionPolicy: Delete

# -- Settings for the block pool snapshot class
# @default -- See [RBD Snapshots](../Storage-Configuration/Ceph-CSI/ceph-csi-snapshot.md#rbd-snapshots)
cephBlockPoolsVolumeSnapshotClass:
  enabled: false
  name: ceph-block
  isDefault: false
  deletionPolicy: Delete

# -- A list of CephObjectStore configurations to deploy
# @default -- See [below](#ceph-object-stores)
cephObjectStores:
  - name: ceph-objectstore
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
      dataPool:
        failureDomain: osd
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
        parameters:
          bulk: "true"
      preservePoolsOnDelete: true
      gateway:
        port: 3333
        # securePort: 443
        # sslCertificateRef:
        instances: 1
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      parameters:
        # note: objectStoreNamespace and objectStoreName are configured by the chart
        region: ch-east-1
    route:
      # Enable an ingress for the ceph-objectstore
      enabled: true
      # The ingress port by default will be the object store's "securePort" (if set), or the gateway "port".
      # To override those defaults, set this ingress port to the desired port.
      # port: 80
      # annotations: {}
      host:
        name: s3.ch-east-1.k8s.garden
        path: /
        pathType: PathPrefix
      parentRefs:
        - name: gateway-shared
          namespace: kube-system
          sectionName: https
## cephECBlockPools are disabled by default, please remove the comments and set desired values to enable it
## For erasure coded a replicated metadata pool is required.
## https://rook.io/docs/rook/latest/CRDs/Shared-Filesystem/ceph-filesystem-crd/#erasure-coded
cephECBlockPools:
  - name: ec-pool
    spec:
      metadataPool:
        replicated:
          size: 2
      dataPool:
        failureDomain: osd
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
        deviceClass: hdd

    parameters:
      # clusterID is the namespace where the rook cluster is running
      # If you change this namespace, also change the namespace below where the secret namespaces are defined
      clusterID: rook-ceph # namespace:cluster
      # (optional) mapOptions is a comma-separated list of map options.
      # For krbd options refer
      # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
      # For nbd options refer
      # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
      # mapOptions: lock_on_read,queue_depth=1024

      # (optional) unmapOptions is a comma-separated list of unmap options.
      # For krbd options refer
      # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
      # For nbd options refer
      # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
      # unmapOptions: force

      # RBD image format. Defaults to "2".
      imageFormat: "2"

      # RBD image features, equivalent to OR'd bitfield value: 63
      # Available for imageFormat: "2". Older releases of CSI RBD
      # support only the `layering` feature. The Linux kernel (KRBD) supports the
      # full feature complement as of 5.4
      # imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
      imageFeatures: layering

    storageClass:
      provisioner: rook-ceph.rbd.csi.ceph.com # csi-provisioner-name
      enabled: true
      name: rook-ceph-block
      isDefault: false
      allowVolumeExpansion: true
      reclaimPolicy: Delete
